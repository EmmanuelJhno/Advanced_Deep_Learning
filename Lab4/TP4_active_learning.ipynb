{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning Practical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active Learning is generally useful in a specific case of small data: when unlabelled data is plentiful, but labelling it is costly. Think for example that each example must be analysed by an expert for labelling it. Expert time is costly, so we would like to be able to train a successful classifier while labelling as few examples as possible.\n",
    "\n",
    "Active Learning is an approach to answer this problem. The general principle is the following: we first randomly ask the expert to label a small random subset of the training data, and train our classifier on it. Once this first step is done, we can analyse the behaviour of our trained model on the yet unlabelled data, to deduce which additional examples would be worth labelling to improve the quality of the model as quickly as possible. We then retrain the model on the new larger labelled dataset and iterate this process, labelling more and more examples, until we reach a satisfying performance. And hopefully reaching it while only labelling a small fraction of the whole available dataset.\n",
    "\n",
    "The main question in Active Learning is thus the following: How should we choose the next batch of examples to be labelled?\n",
    "\n",
    "There are different strategies for this, that you will compare in this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example setup\n",
    "\n",
    "We will emulate an active learning situation using the MNIST dataset, starting by pretending we don't have the labels. The \"ask an expert to label the example\" step will thus be simply picking the label from the dataset.\n",
    "\n",
    "You'll have to provide a neural network architecture and the implementation of various active learning strategies, with the goal of comparing them. Keep in mind that the start of the training will be on very small datasets when designing your architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST dataset from torchvision.dataset\n",
    "original_mnist = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates an unlabelled dataset based on the MNIST images\n",
    "#\n",
    "# All examples that are not yet labelled have a label set to -1\n",
    "def unlabeled_dataset():\n",
    "    return TensorDataset(\n",
    "        original_mnist.data[5000:].float() * 2.0 / 255.0 -1.0,\n",
    "        torch.zeros_like(original_mnist.targets[5000:])-1\n",
    "    )\n",
    "\n",
    "# This function labels the requested examples on an unlabelled dataset\n",
    "def call_expert(dataset, indices):\n",
    "    for i in indices:\n",
    "        dataset.tensors[1][i] = original_mnist.targets[5000+i]\n",
    "\n",
    "# Keep 5000 examples as a validation\n",
    "mnist_validation = TensorDataset(\n",
    "    original_mnist.data[:5000].float() * 2.0 / 255.0 -1.0,\n",
    "    original_mnist.targets[:5000]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can here define your model that will be used for the Active learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.view(-1, 28*28)\n",
    "        output = self.fc1(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the learning hyper-parameters, as for the previous practicals you may need to tinker with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times the expert will be called to label a batch of examples\n",
    "num_active_runs = 20\n",
    "\n",
    "# number of training iterations between each expert call\n",
    "# the number of epochs is thus num_iter * batch_size / len(dataset)\n",
    "num_iter = 100\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the place where you define the strategy to choose the next examples to label. A strategy is a function taking as input your model and the dataset, and returning a list of examples to label.\n",
    "\n",
    "You can create several functions to test different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_strategy(model, dataset):\n",
    "    # label 100 new examples at random\n",
    "    N = len(dataset)\n",
    "    return torch.randint(0, N, (100,))\n",
    "\n",
    "def my_awesome_strategy(model, dataset):\n",
    "    # implement your strategies as new functions here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the main training loop, the only part you should need to modify is the initialization of the optimizer (noted by the `# OPTIMIZER` comment).\n",
    "\n",
    "Don't forget to reload this cell whenever you change the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tensor of values:\n",
    "# - output[:,0] are the number of labeled examples\n",
    "# - output[:,1] are the associated accuracies\n",
    "# - output[:,2] are the associated losses\n",
    "def active_training(strategy, dataset=None, initial_labeling=20):\n",
    "    if dataset is None:\n",
    "        dataset = unlabeled_dataset()\n",
    "        call_expert(dataset, torch.randint(0, len(dataset), (initial_labeling,)))\n",
    "    \n",
    "    def train_model(model, optimizer, loader, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for (data, targets) in loader:\n",
    "                model.zero_grad()\n",
    "                prediction = model(data)\n",
    "                loss = criterion(prediction, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def validation_perf(model, dataset):\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        total_accuracy = 0.0\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for (data, targets) in loader:\n",
    "                prediction = model(data)\n",
    "                total_loss += criterion(prediction, targets).item() * data.size(0)\n",
    "                _, number_predicted = torch.max(prediction.data, 1)\n",
    "                total_accuracy += (number_predicted == targets).sum()\n",
    "        return (total_accuracy / len(dataset), total_loss / len(dataset))\n",
    "    \n",
    "    performances = torch.zeros(num_active_runs+1, 3)\n",
    "    \n",
    "    for i in range(num_active_runs+1):\n",
    "        # filter the dataset to only keep labelled examples\n",
    "        labelled_idx = dataset.tensors[1] >= 0\n",
    "        filtered_dataset = TensorDataset(*list(t[labelled_idx] for t in dataset.tensors))\n",
    "        loader = DataLoader(filtered_dataset, batch_size=batch_size, shuffle=True)\n",
    "        # create a new model to train on this dataset\n",
    "        model = Model()\n",
    "        # OPTIMIZER\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        train_model(model, optimizer, loader, num_iter // len(loader))\n",
    "        (valid_acc, valid_loss) = validation_perf(model, mnist_validation)\n",
    "        \n",
    "        performances[i, 0] = len(filtered_dataset)\n",
    "        performances[i, 1] = valid_acc\n",
    "        performances[i, 2] = valid_loss\n",
    "        print(\"With {} examples, valid perf is: {} -- {}%\".format(len(filtered_dataset), valid_loss, valid_acc*100))\n",
    "        \n",
    "        if i < num_active_runs:\n",
    "            to_label = strategy(model, dataset)\n",
    "            call_expert(dataset, to_label)\n",
    "    \n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells contain code that you can use to compare your different strategies. Fill the `strategies` list with the name of the functions implementing your strategies, and run the cell to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of strategies you want to compare\n",
    "strategies = [random_strategy]\n",
    "# number of examples to label randomly at the start\n",
    "initial_labeling = 100\n",
    "\n",
    "performances = []\n",
    "for strat in strategies:\n",
    "    print(\"Training with strategy: {}\".format(strat.__name__))\n",
    "    perf = active_training(strat, initial_labeling=initial_labeling)\n",
    "    performances.append((strat.__name__, perf))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Compared validation accuracies\")\n",
    "plt.xlabel(\"Number of labeled examples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "for (name, perf) in performances:\n",
    "    plt.plot(perf[:,0], perf[:,1], label=name)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Compared validation losses\")\n",
    "plt.xlabel(\"Number of labeled examples\")\n",
    "plt.ylabel(\"Loss\")\n",
    "for (name, perf) in performances:\n",
    "    plt.plot(perf[:,0], perf[:,2], label=name)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code only implement the \"random\" strategy, which chooses examples at random to label them. Your task is to compare it to other, more guided strategies. For example:\n",
    "\n",
    "- a strategy that would label in priority the examples for which the classifier is the most uncertain, as measured by the entropy of its output\n",
    "- a strategy that would label in priority the examples that appear to be near the frontier between the classes: examples for which the highest and second-highest prediction values of the classifier are close to each other.\n",
    "\n",
    "You can also try to use other relevant decision criterion for choosing which examples to label, and implement them as new strategies. It is also possible to combine strategies together, in order to combine the strong points of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some advice:**\n",
    "\n",
    "Keep in mind that your goal is to reach the best possible validation accuracy with as little labelled training examples as possible. As such, don't forget to try and change the number of initial examples for labelling, as well as the number of examples your strategy will request for labelling at each round.\n",
    "\n",
    "Also keep in mind that you network will be trained on possibly very small datasets (think of only a few hundred of examples), while the validation set contains 5000 examples. Be careful of the risks of over-fitting, adjust your network structure and the training time (`num_iter`) appropriately.\n",
    "\n",
    "As a baseline, you should be able to reach at least 90% of validation accuracy with less than 500 labelled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you are done, you're asked to upload this notebook with your model, the different strategies your compared with the graphs generated by the previous code. Your strategies functions should be commented to explain what each strategy does, and which one is your best strategy.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
