{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task problem: colored MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colored [MNIST Dataset](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)\n",
    "* Handwritten digits with 10 classes\n",
    "* Size of each image: 28x28 pixels \n",
    "* 50 000 data examples in training set, 10 000 examples in validation set, 10 000 in test set\n",
    "* We colorize each image with a random color within 7 (red, green, blue, magenta, yellow, cyan, white)\n",
    "* Each image has two labels: the number it represents (10 classes) and the color of the number (7 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a loaded MNIST dataset, create a colozied version of it\n",
    "def colorize_dataset(dataset):\n",
    "    # array of colors\n",
    "    COLORS = torch.tensor([\n",
    "        [1.0, 0.0, 0.0], # 0 RED\n",
    "        [0.0, 1.0, 0.0], # 1 GREEN\n",
    "        [0.0, 0.0, 1.0], # 2 BLUE\n",
    "        [1.0, 1.0, 0.0], # 3 YELLOW\n",
    "        [1.0, 0.0, 1.0], # 4 MAGENTA\n",
    "        [0.0, 1.0, 1.0], # 5 CYAN\n",
    "        [1.0, 1.0, 1.0], # 6 WHITE\n",
    "    ])\n",
    "    N = len(dataset)\n",
    "    images = dataset.data.view(N, 1, 28, 28)\n",
    "    labels = dataset.targets.view(N, 1)\n",
    "    color_labels = torch.randint(0, 6, (N,))\n",
    "    colorized_images = images * COLORS[color_labels, :].view(N,3,1,1)\n",
    "    full_labels = torch.cat((labels, color_labels.view(N, 1)), dim=1)\n",
    "    return TensorDataset(colorized_images, full_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2159824.95it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 126159.77it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 868861.92it/s]                              \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 47500.85it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Loading MNIST dataset from torchvision.dataset\n",
    "dataset = torchvision.datasets.MNIST(root='data/',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "dataset = colorize_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is : torch.Size([60000, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of the dataset is :\", dataset.tensors[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation sets\n",
    "train_set, val_set = random_split(dataset, [50000, 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'image label: tensor([9, 2])')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 80\n",
    "\n",
    "plt.imshow(dataset[sample_index][0].permute(1, 2, 0), interpolation='nearest')\n",
    "plt.title(\"image label: {}\".format(dataset[sample_index][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = nn.Linear(28*28*3, 28*28*6)\n",
    "        self.l2 = nn.Linear(28*28*6, 28*28*6)\n",
    "        # The input size is 28*28. For a standard classification task, the output size should be the same as the number of classes\n",
    "        self.l_number = nn.Linear(28*28*6, 10)\n",
    "        # However here we deal with 2 tasks: the network outputs 2 labels, so there are two \"last\" layers in parallel\n",
    "        self.l_color = nn.Linear(28*28*6, 7)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = inputs.view(-1, 28*28*3)\n",
    "        h = torch.relu(self.l1(h))\n",
    "        \n",
    "        h = self.l2(h)\n",
    "        h = torch.relu(self.l2(h))\n",
    "        # Use softmax as the activation function for the last layer(s)\n",
    "        output_number = F.softmax(self.l_number(h), dim=1)\n",
    "        output_color = F.softmax(self.l_color(h), dim=1)\n",
    "        \n",
    "        return (output_number, output_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: \n",
    "model = Model()\n",
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Use mean squared loss function \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for training\n",
    "def train(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
    "    train_error = []\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_average_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            (y_number, y_color) = model(images)\n",
    "            \n",
    "            # One-hot encoding or labels so as to calculate MSE error:\n",
    "            number_onehot = F.one_hot(labels[:,0], 10).float()\n",
    "            color_onehot = F.one_hot(labels[:,1], 7).float()\n",
    "            \n",
    "            loss = criterion(y_number, number_onehot) + criterion(y_color, color_onehot)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "        train_error.append(epoch_average_loss)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
    "    return train_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0464c72c26ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-89ad2e5c83e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_onehot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mepoch_average_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training error wrt. the number of epochs: \n",
    "plt.plot(range(1, num_epochs+1), train_error)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")\n",
    "plt.savefig('3_dense_layer_conv.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy to evaluate the model\n",
    "def accuracy(dataset, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        number_correct = 0\n",
    "        color_correct = 0\n",
    "        both_correct = 0\n",
    "        dataloader = DataLoader(dataset)\n",
    "        for images, labels in dataloader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            (y_number, y_color) = model(images)\n",
    "            _, number_predicted = torch.max(y_number.data, 1) \n",
    "            number_correct += (number_predicted == labels[:, 0]).sum()\n",
    "            _, color_predicted = torch.max(y_color.data, 1) \n",
    "            color_correct += (color_predicted == labels[:, 1]).sum()\n",
    "            both_correct += ((color_predicted == labels[:, 1]) and (number_predicted == labels[:, 0])).sum()\n",
    "\n",
    "    print('Accuracy of the model for numbers : {:.2f} %'.format(100*number_correct.item()/ len(dataset)))\n",
    "    print('Accuracy of the model for colors : {:.2f} %'.format(100*color_correct.item()/ len(dataset)))\n",
    "    print('Accuracy of the model for both : {:.2f} %'.format(100*both_correct.item()/ len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model for numbers : 46.25 %\n",
      "Accuracy of the model for colors : 33.05 %\n",
      "Accuracy of the model for both : 15.67 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(val_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Prediction: number=9, color=3')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_index = 66\n",
    "\n",
    "(image, label) = val_set[val_index]\n",
    "(y_number, y_color) = model(image)\n",
    "_, number_prediction = torch.max(y_number.data, 1)\n",
    "_, color_prediction = torch.max(y_color.data, 1)\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0), interpolation='nearest')\n",
    "plt.title(\"Prediction: number=%d, color=%d\" % (number_prediction, color_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the architecture of the model\n",
    "Define your own class `Model` to improve the predictions:\n",
    "\n",
    "* The convolutional layer can be a good choice to deal with images. Replace nn.Linear with [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d).\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Change the number of neurons in hidden layers (5, 10, 20, more ?)\n",
    "* Try different activation functions such as [sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc.\n",
    "* __Your network generates two different outputs, how much weight-sharing (i.e. how many common layers) between these two paths is appropriate?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.l_number = nn.Flatten()\n",
    "        self.l_color = nn.Flatten()\n",
    "        # The input size is 28*28. For a standard classification task, the output size should be the same as the number of classes\n",
    "        self.l_number = nn.Conv2d(3,16,5)\n",
    "        # However here we deal with 2 tasks: the network outputs 2 labels, so there are two \"last\" layers in parallel\n",
    "        self.l_color = nn.Conv2d(3,16,5)\n",
    "        self.l_number = nn.Linear(16*28*28, 10)\n",
    "        self.l_color = nn.Linear(16*28*28, 7)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = inputs.view(-1, 28*28*3)\n",
    "        # Use softmax as the activation function for the last layer(s)\n",
    "        output_number = F.relu(self.l_number(h), dim=1)\n",
    "        output_color = F.softmax(self.l_color(h), dim=1)\n",
    "        \n",
    "        return (output_number, output_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model: \n",
    "model = Model1()\n",
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Use mean squared loss function \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10 x 2352], m2: [12544 x 10] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-0464c72c26ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-89ad2e5c83e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mepoch_average_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0my_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_color\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# One-hot encoding or labels so as to calculate MSE error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-c1a30f94a89a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Use softmax as the activation function for the last layer(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moutput_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [10 x 2352], m2: [12544 x 10] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerModel(nn.Module):\n",
    "    def __init__(self, n_hidden_layers=0, n_neurons=2, hidden_activations=torch.relu, \n",
    "                 output_activation=torch.softmax):\n",
    "        super(MultiLayerModel, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_hidden_layers\n",
    "        \n",
    "        if type(n_neurons)==int:\n",
    "            self.n_neurons = [3] + [n_neurons for _ in range(n_hidden_layers)]\n",
    "        else:\n",
    "            assert type(n_neurons)==list\n",
    "            assert len(n_neurons)==n_hidden_layers\n",
    "            self.n_neurons = [3] + n_neurons\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            input_dim = self.n_neurons[i]\n",
    "            output_dim = self.n_neurons[i+1]\n",
    "            setattr(self,\n",
    "                    \"layer_{}\".format(i+1),\n",
    "                    nn.Conv2d(input_dim, output_dim))\n",
    "        \n",
    "        self.l_number = nn.Conv2d(self.n_neurons[-1], 16,(3,3))\n",
    "        self.l_color = nn.Conv2d(self.n_neurons[-1], 16,(3,3))\n",
    "        \n",
    "        self.hidden_activations = hidden_activations\n",
    "        self.output_activation = output_activation\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for i in range(self.n_layers):\n",
    "            x = getattr(self, \"layer_{}\".format(i+1))(x)\n",
    "            if i < self.n_layers - 1:\n",
    "                x = self.hidden_activations(x)\n",
    "        output_number = self.output_activation(self.l_number(x))\n",
    "        output_color = self.output_activation(self.l_color(x))\n",
    "        \n",
    "        \n",
    "        return (output_number, output_color)\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers=['dense_512'], with_softmax=True, hidden_activation=torch.relu):\n",
    "        super().__init__()\n",
    "        assert isinstance(layers, list)\n",
    "        for layer in layers: assert isinstance(layer, str) \n",
    "        self.with_softmax = with_softmax\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.n_hidden_layers = len(layers)\n",
    "        self.input_shapes = None\n",
    "        self.needs_input_flat = True\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            params = layer.split('_')\n",
    "            layer_type, params = params[0], params[1:]\n",
    "            assert layer_type in ['dense', 'conv2d', 'maxpool2d', 'flatten']\n",
    "            \n",
    "            \n",
    "            if layer_type == 'dense':\n",
    "                if i == 0:\n",
    "                    self.needs_input_flat = True\n",
    "                    self.input_shapes = [28*28]\n",
    "                n_neurons = int(params[0])\n",
    "                in_shape = self.input_shapes[-1]\n",
    "                layer = nn.Linear(in_shape, n_neurons)\n",
    "                self.input_shapes.append(n_neurons)\n",
    "            \n",
    "            elif layer_type == 'conv2d':\n",
    "                if i == 0:\n",
    "                    self.needs_input_flat = False\n",
    "                    self.input_shapes = [(28,28,1)]\n",
    "                in_shape = self.input_shapes[-1]\n",
    "                assert isinstance(in_shape, tuple)\n",
    "                assert len(in_shape) == 3\n",
    "                in_width, in_height, in_channels = in_shape\n",
    "                n_channels, kernel_size, stride = int(params[0]), int(params[1]), int(params[2])\n",
    "                layer = nn.Conv2d(in_channels=in_channels,\n",
    "                                  out_channels=n_channels, \n",
    "                                  kernel_size=kernel_size, \n",
    "                                  stride=stride)\n",
    "                self.input_shapes.append(\n",
    "                    (floor((in_width - kernel_size)/stride + 1),\n",
    "                     floor((in_height - kernel_size)/stride + 1),\n",
    "                     n_channels\n",
    "                    )\n",
    "                )\n",
    "            elif layer_type == 'flatten':\n",
    "                layer = nn.Flatten()\n",
    "                in_shape = self.input_shapes[-1]\n",
    "                out_shape = in_shape[0]*in_shape[1]*in_shape[2]\n",
    "                self.input_shapes.append(out_shape)\n",
    "            \n",
    "            elif layer_type == 'maxpool2d':\n",
    "                in_shape = self.input_shapes[-1]\n",
    "                assert isinstance(in_shape, tuple)\n",
    "                assert len(in_shape) == 3\n",
    "                in_width, in_height, _ = in_shape\n",
    "                \n",
    "                kernel_size = int(params[0])\n",
    "                layer = nn.MaxPool2d(kernel_size=kernel_size)\n",
    "                self.input_shapes.append(\n",
    "                        (floor((in_width - kernel_size)/kernel_size + 1),\n",
    "                         floor((in_height - kernel_size)/kernel_size + 1),\n",
    "                         n_channels\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            setattr(self, 'hidden_{}'.format(i), layer)\n",
    "            \n",
    "        self.l_number = nn.Linear(self.input_shapes[-1], 10)\n",
    "        self.l_color = nn.Linear(self.input_shapes[-1], 7)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            layer = getattr(self, \"hidden_{}\".format(i))\n",
    "            x = self.hidden_activation(layer(x))\n",
    "            \n",
    "        x_number = self.l_number(x) \n",
    "        x_color = self.l_color(x)\n",
    "        if self.with_softmax:\n",
    "            x_number = F.softmax(x_number, dim=1)\n",
    "            x_color = F.softmax(x_color, dim=1)\n",
    "        return (x_number,x_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(model_name, train_error, val_acc):\n",
    "    plt.plot(range(1, len(train_error) + 1), train_error)\n",
    "    plt.title(\"Train error for {} - Validation accuracy: {:.2f}%\".format(model_name, val_acc*100))\n",
    "    plt.xticks(range(1, len(train_error) + 1))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    \n",
    "    \n",
    "def plot_image(model, val_set, val_index=66):\n",
    "    (image, label) = val_set[val_index]\n",
    "    image = image.to(device)\n",
    "    if model.needs_input_flat:\n",
    "        image = image.view(-1, 28*28*3)\n",
    "    else:\n",
    "        image = image.view(-1, 3, 28, 28)\n",
    "    output = model(image)\n",
    "    _, prediction = torch.max(output.data, 1)\n",
    "    plt.imshow(image.cpu().view(3, 28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Prediction label: %d\" % prediction)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:  1 layer, 50 neurons\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [8400 x 28], m2: [784 x 50] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-4814f36ed787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training model: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-89ad2e5c83e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mepoch_average_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0my_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_color\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# One-hot encoding or labels so as to calculate MSE error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-d04fb9cedfac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [8400 x 28], m2: [784 x 50] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "epochs = 8\n",
    "batch_size = 100\n",
    "\n",
    "architectures = {\n",
    "    \"1 layer, 50 neurons\": ([\"dense_50\"]*1, 0.05),\n",
    "    \"3 layers, 50 neurons\": ([\"dense_50\"]*3, 0.1),\n",
    "    \"5 layers, 50 neurons\": ([\"dense_50\"]*5, 0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, (arch, lr) in architectures.items():\n",
    "    model = CustomModel(layers=arch, with_softmax=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    loss = nn.MSELoss()\n",
    "    print(\"Training model: \", name)\n",
    "    train_errors = train(num_epochs, batch_size, criterion, optimizer, model, train_set)\n",
    "    val_acc = compute_accuracy(model, val_set)\n",
    "    plt.subplot(121)\n",
    "    plot_convergence(model_name=name, train_error=train_errors, val_acc=val_acc)\n",
    "    plt.subplot(122)\n",
    "    plot_image(model=model, val_set=val_set)\n",
    "    plt.show()\n",
    "    print(\"-\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model used is :\n",
      " MultiLayerModel(\n",
      "  (l_number): Conv2d(2352, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (l_color): Conv2d(2352, 7, kernel_size=(3, 3), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size 10 2352 3 3, expected input[10, 3, 28, 28] to have 2352 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-3d070a8a325e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-89ad2e5c83e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, criterion, optimizer, model, dataset)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mepoch_average_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0my_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_color\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# One-hot encoding or labels so as to calculate MSE error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-029061c7a480>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0moutput_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 10 2352 3 3, expected input[10, 3, 28, 28] to have 2352 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "n_hidden_layers=0\n",
    "n_neurons=4\n",
    "\n",
    "# Choose the hyperparameters for training: \n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "# Training criterion. This one is a mean squared error (MSE) loss between the output\n",
    "# of the network and the target label\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate of 0.01\n",
    "# It is initialized on our model\n",
    "model = CustomModel(n_hidden_layers, n_neurons)\n",
    "print('The model used is :\\n', model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the optimizer\n",
    "Retrain the model by using different parameters of the optimizer; you can change its parameters in the cell initializing it, after the definition of your model.\n",
    "\n",
    "* Use different batch sizes, from 10 to 1 000 for instance\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "The MSE error is rarely used in this case. The cross entropy loss can be a better choice for multi-classification problems. In pytorch, the cross entropy loss is defined by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Replace the MSE loss by this one to observe its impact.\n",
    "\n",
    "**Note:** In order to use nn.CrossEntropyLoss correctly, don't add an activation function to the last layer of your network. And one-hot encoding is no longer needed to calculate the loss, delete the encoding procedures in function `train`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST test set from torchvision.dataset\n",
    "test_set = torchvision.datasets.MNIST(root='data/',\n",
    "                                         train=False,\n",
    "                                         transform=transforms.ToTensor(),\n",
    "                                         download=False)\n",
    "test_set = colorize_dataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
